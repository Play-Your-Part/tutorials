{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhVhK7adbAMY"
      },
      "source": [
        "# 🪝 Mechanistic Interpretability Notebook (with answers 🙈) (JSALT 25) 🪝"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOfcwp2Xd8qa"
      },
      "source": [
        " This is a tutorial (adapted from https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Main_Demo.ipynb#scrollTo=UYDxNbg4ap-b) to study how a LM model reacts internally when stimulated by prompts.\n",
        "\n",
        " The tutorial is supposed to be independent with regard to the Language Model (LM) we choose to work on.\n",
        "\n",
        "We will be using **TransformerLens**, this is a really cool library designed for mechanistic interpretability 🤯 .\n",
        "\n",
        "It supports LMs from HuggingFace and provide powerful visualization tools and easy way to track activations inside the model through the use of custom hooks 🪝🤖 .\n",
        "\n",
        "In the first time, we will learn all the necessary functions/methods to :\n",
        "\n",
        "* **Extract the internal representations of the LM when faced with prompts.**\n",
        "\n",
        "* **Visualize the attention patterns.**\n",
        "\n",
        "* **Modify the internal representations duiring inference, to alter the response of the LM using custom hooks.**\n",
        "\n",
        "Then, we will use all of these tools to reproduce the results of a specific paper related to mechanistic interpretability.\n",
        "\n",
        "#### Preparations (if you are using Google Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U-R1atDd632"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "DEVELOPMENT_MODE = False\n",
        "# Detect if we're running in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Install if in Colab\n",
        "if IN_COLAB:\n",
        "    %pip install transformer_lens\n",
        "    %pip install circuitsvis\n",
        "    # Install a faster Node version\n",
        "    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs  # noqa\n",
        "\n",
        "# Hot reload in development mode & not running on the CD\n",
        "if not IN_COLAB:\n",
        "    from IPython import get_ipython\n",
        "    ip = get_ipython()\n",
        "    if not ip.extension_manager.loaded:\n",
        "        ip.extension_manager.load('autoreload')\n",
        "        %autoreload 2\n",
        "\n",
        "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
        "\n",
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import plotly.io as pio\n",
        "if IN_COLAB or not DEVELOPMENT_MODE:\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\"\n",
        "print(f\"Using renderer: {pio.renderers.default}\")\n",
        "\n",
        "import circuitsvis as cv\n",
        "# Testing that the library works\n",
        "cv.examples.hello(\"Neel\")\n",
        "# import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, FactoredMatrix\n",
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import tqdm.auto as tqdm\n",
        "import plotly.express as px\n",
        "from IPython.display import display\n",
        "from transformer_lens.utils import get_act_name\n",
        "\n",
        "from jaxtyping import Float\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7LymG8PUen7"
      },
      "source": [
        "# Learning TransformerLens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGzBpMq8eqvq"
      },
      "source": [
        "In this section, we are going to learn how to use the different tools available to us (thanks to TransformerLens). visualize how our model reacts to a very simple query :\n",
        "\n",
        "**\"The capital of France is\"**\n",
        "\n",
        " A typical Language Model leverages internal attention mechanisms in order to understand the context of a given sentence. These attentions activations, located inside each Transformer, vary depending on the layer we want to study.\n",
        "\n",
        "Let's first visualize how a simple Language Model contextually attends to the mentionned query.\n",
        "\n",
        "To do so, we are going to load a small pretrained model (`bloom` for now), using the **TransformerLens** wrapper and running a **forward pass** with our **query**.\n",
        "\n",
        "As the `HookedTransformer` class already disposes of **default hooks** attached to the attention layers of **bloom**, we are not going to need to add any hooks (yet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq3AvTQbe2NO"
      },
      "outputs": [],
      "source": [
        "# Let's load a Language Model\n",
        "# There are more than 40 supported models, the full list is available here : https://github.com/TransformerLensOrg/TransformerLens/blob/47fe15666017d1b507bfebeefd877dbc428d8463/transformer_lens/loading_from_pretrained.py#L66)\n",
        "\n",
        "device = utils.get_device() # Setting the device\n",
        "\n",
        "model = HookedTransformer.from_pretrained(\"bigscience/bloom-560m\", device=device) # Load the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPTZcSAakb5W"
      },
      "source": [
        "Looking at the different layers of the model, you will see that it has custom hook classes hooks attached to it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjMnQmrzkk1P"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8r3UL81dylM"
      },
      "source": [
        "### How to generate a response ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNHyX74YfPUz"
      },
      "source": [
        "Unfortunately, TransformerLens does not come with a built-in response generation function. So we are going to build a simple one using greedy decoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj1wfTVRfV8y"
      },
      "outputs": [],
      "source": [
        "# Generate a response from a given LM\n",
        "def generate_response(model, prompt, max_new_tokens=100):\n",
        "    tokens = model.to_tokens(prompt) # Convert the prompt to tokens\n",
        "    generated = tokens.clone()\n",
        "    for _ in range(max_new_tokens): # Loop to feed the prompt and the newly generated tokens\n",
        "        logits = model(generated)\n",
        "        next_token = logits[0, -1].argmax(dim=-1, keepdim=True)  # greedy decoding\n",
        "        generated = torch.cat([generated, next_token.unsqueeze(0)], dim=-1)\n",
        "    #answer_tokens = generated[0, tokens.shape[-1]:]  # exclude the prompt length\n",
        "    answer = model.to_string(generated[0]) # Convert the generated tokens back to text\n",
        "    return generated, answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ehe9ep9blN1"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "💡 Tips 💡\n",
        "\n",
        "*   For each prompt, we must tokenize the given string. Each model comes with his built-in tokenizer method `model.to_tokens()`. Same goes for when we need to convert the **tokens** back to text using `model.to_string()`.\n",
        "\n",
        "*   We want to be able to change the number of tokens generated by the model if we change `max_new_tokens` parameter.\n",
        "\n",
        "*   Getting the output logit of the next token is done by using the forward pass of the class associated to the model `model()`. Remember that the model expects tokens (and not strings).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAmYcnd9wAnB"
      },
      "source": [
        "For now, let's first generate a response from the LM for a given prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI53MWHgksMB"
      },
      "outputs": [],
      "source": [
        "prompt = \"The capital of France is\"\n",
        "\n",
        "answer_token, generated_response = generate_response(model, prompt, max_new_tokens=15) # Here we will get the sequence of tokens that are predicted by the LM, as well as a direct converted string -> text version\n",
        "# The maximum number of tokens generated determines the length of the response. Let's keep it at 15 for now.\n",
        "print(\"Response : \",generated_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFRWRVp7fkcN"
      },
      "source": [
        "### How to visualize internal activations ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3Pr71_nww0j"
      },
      "source": [
        "TransformerLens allows us to internally look at how each attention head contributes w.r.t each tokens.\n",
        "For a given layer, we can look at these contributions very easily.\n",
        "\n",
        "*   The `model.run_with_cache()` method is one of the key features of TransformerLens. It allows us to return a forward pass and store the **cache** activations of the entire forward of the model. This is very useful when we want to look at specific representations of the model.\n",
        "\n",
        "*   It returns the `logits` and a `cache` variable.\n",
        "\n",
        "*   `cache` is an `ActivationCache` object, which internally stores hooked activations using human-readable or structured names like:\n",
        "\n",
        "  `'blocks.0.attn.hook_pattern'`\n",
        "\n",
        "  `'blocks.0.hook_resid_pre'`\n",
        "\n",
        "  etc...\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R91wuL_1pe6D"
      },
      "outputs": [],
      "source": [
        "prompt = \"The capital of France is\"\n",
        "\n",
        "tokens = model.to_tokens(prompt) # Tokenize it\n",
        "logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
        "\n",
        "print(\"Cache keys:\")\n",
        "for key in cache.keys():\n",
        "    print(\" -\", key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzKXJMHJpzF0"
      },
      "source": [
        "One can simply access the internal representations at some point by selecting the appropriate key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhXexFa8p79C"
      },
      "outputs": [],
      "source": [
        "# Let us access the attention query of the 23rd Transformer Layer.\n",
        "\n",
        "cache[\"blocks.23.attn.hook_q\"].size() #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8XV1O2YvXXz"
      },
      "source": [
        "Let's now try to extract the `SOFTMAXED ATTENTION SCORE` of any INPUT PROMPT using the `run_with_cache()` for a given layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AcgLIDxgzsC"
      },
      "outputs": [],
      "source": [
        "# Run a forward and/or visualize the attention pattern for a given\n",
        "def run_and_visualize_attention_patterns(model,prompt,layer, visualize=True):\n",
        "  tokens = model.to_tokens(prompt) # Tokenize it\n",
        "  logits, cache = model.run_with_cache(tokens, remove_batch_dim=True) # And run the model with cache activation of the attention layers\n",
        "  attention_pattern = cache[\"pattern\", layer, \"attn\"] # Fetch the softmax(QKt/sqrt(d_k)) of each attention block\n",
        "  str_tokens = model.to_str_tokens(prompt) # We need a string list of each token []\n",
        "  print(tokens)\n",
        "  print(str_tokens)\n",
        "  if visualize :\n",
        "    print(\"Number of tokens : \", len(tokens[0]))\n",
        "    print(f\"Layer {layer} Head Attention Patterns for prompt :\")\n",
        "    fig = cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern)\n",
        "    display(fig)\n",
        "\n",
        "  return(logits,cache)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNQEjZBmgu8f"
      },
      "source": [
        "💡 Tips 💡\n",
        "\n",
        "*    Visualization will be done automatically by the `circuitvis` library (already coded :) ).\n",
        "\n",
        "*    For a given layer (23 for example) :\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "┌──────────────────────────────────────────────┐\n",
        "│ blocks.23.hook_resid_pre                     │ ◄── Input to block 23\n",
        "└──────────────────────────────────────────────┘\n",
        "                  │\n",
        "                  ▼\n",
        "┌──────────────────────────────────────────────┐\n",
        "│ blocks.23.ln1.hook_scale                     │ ◄── LayerNorm scale\n",
        "└──────────────────────────────────────────────┘\n",
        "                  │\n",
        "                  ▼\n",
        "┌──────────────────────────────────────────────┐\n",
        "│ blocks.23.ln1.hook_normalized                │ ◄── LayerNorm output\n",
        "└──────────────────────────────────────────────┘\n",
        "                  │\n",
        "                  ▼\n",
        "┌─────────────────────────────────────────────────────────────┐\n",
        "│ blocks.23.attn.hook_q / hook_k / hook_v                     │ ◄── Q, K, V projections\n",
        "└─────────────────────────────────────────────────────────────┘\n",
        "                  │\n",
        "                  ▼\n",
        "┌──────────────────────────────────────────────┐\n",
        "│ blocks.23.attn.hook_attn_scores              │ ◄── QKᵀ / √d\n",
        "└──────────────────────────────────────────────┘\n",
        "                  │\n",
        "                  ▼\n",
        "┌──────────────────────────────────────────────┐\n",
        "│ blocks.23.attn.hook_pattern                  │ ◄── Softmax(QKᵀ/√d)\n",
        "└──────────────────────────────────────────────┘\n",
        "                  │\n",
        "                  ▼\n",
        "┌──────────────────────────────────────────────┐\n",
        "│ blocks.23.attn.hook_z                        │ ◄── ∑ (Attn Weights × V)\n",
        "└──────────────────────────────────────────────┘\n",
        "                  │\n",
        "                  ▼\n",
        "┌──────────────────────────────────────────────┐\n",
        "│ blocks.23.hook_attn_out                      │ ◄── Output of attn head\n",
        "└──────────────────────────────────────────────┘\n",
        "                  │\n",
        "                  ▼\n",
        "┌──────────────────────────────────────────────┐\n",
        "│ blocks.23.hook_resid_mid                     │ ◄── Residual after attn\n",
        "└──────────────────────────────────────────────┘\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtGQ59Gdf_2P"
      },
      "source": [
        "Let's visualize them for a given layer  (ex : layer 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN9hKksqg0wz"
      },
      "outputs": [],
      "source": [
        "layer = 8\n",
        "logits, cache = run_and_visualize_attention_patterns(model,prompt,layer, visualize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO5RbsgKxCkR"
      },
      "source": [
        "As you can see (and play with the visualization tool CircuitVis), it is possible to see how the model attributes relationships between tokens through the heads.\n",
        "\n",
        "The shape of the visualization (gradually increasing from top to bottom) shows the causal contributions of the seen sequence.\n",
        "\n",
        "While some heads show no relationships at all (head 12 or head 9), others show a strong contribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGZRKgv1R7nt"
      },
      "source": [
        "We can also visualize the attention pattern of the generated tokens. For that, we will first generate the tokens, then rerun the inference with caching (as shown below), with our newly generated tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ba2eoB6_MYX"
      },
      "outputs": [],
      "source": [
        "def run_and_visualize_attention_patterns_with_answer(model, prompt, layer, max_new_tokens=20, visualize=True):\n",
        "    # Step 1: Generate tokens (prompt + answer)\n",
        "    full_tokens, answer = generate_response(model, prompt, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # Step 2: Run the full sequence with caching\n",
        "    logits, cache = model.run_with_cache(full_tokens, remove_batch_dim=True)\n",
        "\n",
        "    # Step 3: Convert tokens to strings\n",
        "    str_tokens = model.to_str_tokens(full_tokens)\n",
        "\n",
        "    # Step 4: Extract attention patterns\n",
        "    attention_pattern = cache[\"pattern\", layer, \"attn\"]  # Shape: [n_heads, seq, seq]\n",
        "\n",
        "    if visualize:\n",
        "        #print(\"Prompt + Answer:\", model.to_string(full_tokens[0]))\n",
        "        #print(f\"Layer {layer} Head Attention Patterns:\")\n",
        "        fig = cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern)\n",
        "        display(fig)\n",
        "\n",
        "run_and_visualize_attention_patterns_with_answer(model, prompt, layer, max_new_tokens=20, visualize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9x8gX0dyD9u"
      },
      "source": [
        "💡 Tips 💡\n",
        "\n",
        "*    One can re-use the previous functions (the one we used to generate the answer using greedy decoding), to get the expected output tokens.\n",
        "\n",
        "*    And then, run the inference again with cache using the predicted tokens as our new input prompts, to get the activation patterns. (`run_with_cache()`)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZq0AAfVq28W"
      },
      "source": [
        "As you can see for layer 8, the Head 1 (aka the second head) highly associates the \"capital\" token, to each capital."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4JDjg8zUtQN"
      },
      "source": [
        "# Activation manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1PbikkcXcNf"
      },
      "source": [
        "In this section, we are going to play with the activations to alter the answer of the LM.\n",
        "\n",
        "In our example, let's take our previous prompt :\n",
        "\n",
        "**\"The capital of France is\"**\n",
        "\n",
        "The result from such a prompt would be : **\"Paris\"**.\n",
        "\n",
        "But what if we wanted to make the Language Model hallucinate and give it another type of **capital** : **\"Madrid\"** for example.\n",
        "\n",
        "💡 To get such a result, we need to know how the model internally behaves towards this specific wrong answer and the residual stream. 💡\n",
        "\n",
        "To do so, a typical way would be to run the inference on the LM using both a clean and corrupt version of the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAkf-4ogzfRj"
      },
      "outputs": [],
      "source": [
        "# Create direction: force the model to say \"Madrid\" instead of \"Paris\"\n",
        "clean_prompt = \"The capital of France is Paris\" # Clean prompt\n",
        "corrupt_prompt = \"The capital of France is Madrid\" # Corrupted prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8aV2ACzzjGG"
      },
      "source": [
        "Now, let's create the small function that will give us this direction. To do so, we will extract the internal representations of one of the Transformer layer's output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiiXYa4NfeyG"
      },
      "outputs": [],
      "source": [
        "# Build residual steering direction (Paris → Madrid)\n",
        "def get_residual_diff(model, prompt_a, prompt_b, layer=10, pos=-1):\n",
        "    toks_a = model.to_tokens(prompt_a)\n",
        "    toks_b = model.to_tokens(prompt_b)\n",
        "\n",
        "    _, cache_a = model.run_with_cache(toks_a)\n",
        "    _, cache_b = model.run_with_cache(toks_b)\n",
        "\n",
        "    # To get the name of the residual stream hook, we use an utility function from TransformerLens called \"get_act_name\"\n",
        "    resid_a = cache_a[get_act_name(\"resid_post\", layer)][0, pos]\n",
        "    resid_b = cache_b[get_act_name(\"resid_post\", layer)][0, pos]\n",
        "\n",
        "    return resid_b - resid_a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeXGkyDZfqHV"
      },
      "source": [
        "💡 Tips 💡\n",
        "\n",
        "*  Since the `run_with_cache()` method stores these residuals, it is easy to access them.\n",
        "\n",
        "\n",
        "*  Since we want to alter the answer (PARIS -> MADRID), we look at the last token generated (position **-1**).\n",
        "\n",
        "*  `resid_post` represents the final output of a Transformer layer. For simplicity, we can get the name of the layer using the `get_act_name` function.\n",
        "   ex :\n",
        "   \n",
        "  \n",
        "\n",
        "```\n",
        "get_act_name(\"resid_post\", 8) -> blocks.8.hook_resid_post\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOQa_YxMg8KA"
      },
      "outputs": [],
      "source": [
        "direction = get_residual_diff(model, clean_prompt, corrupt_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEDLT9UkhBqL"
      },
      "source": [
        "Alright ! We have identified our direction.\n",
        "\n",
        "Now, we can intervene on the Language Model to add the \"Madrid\" **direction**.\n",
        "\n",
        "To do that, we are going to use **custom hooks** that we are going to attach to the Language Model during the inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oia9JyyYiSaB"
      },
      "outputs": [],
      "source": [
        "# Hook setup\n",
        "def steer_residual_hook(resid, hook, direction, coeff=1.0):\n",
        "    resid[:, -1, :] += coeff * direction # Attend to the last token\n",
        "    return resid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1iqD1P4iUcT"
      },
      "source": [
        "💡 Tips 💡\n",
        "\n",
        "The hook will take as input the residual stream and a hook argument (that is necessary for the hook-calling mechanism of TransformerLens, but we do not need to care about that parameter).\n",
        "\n",
        "\n",
        "\n",
        "*   The function takes in the **direction** and that we want to ADD to the residual stream of the last generated token, to shift the activations.\n",
        "\n",
        "*   The `coeff` argument controls the **strength** of the direction. It should be multiplied by the direction when adding it to the residual stream.\n",
        "\n",
        "*   We want to modify the representations of the FIRST generated token.\n",
        "\n",
        "Once we are done, we can wrap this function into a second function (shown below) that will return the hook. This is necessary to run the inference with specific hooks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5xQvACtjLel"
      },
      "outputs": [],
      "source": [
        "def hook_fn(resid, hook):\n",
        "    return steer_residual_hook(resid, hook, direction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X71HDdebjWnH"
      },
      "source": [
        "Finally, we can run the inference again ! We will apply the direction to Layer 10 (but it could be any other layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAojD00Ljcqp"
      },
      "outputs": [],
      "source": [
        "# Tokenize prompt\n",
        "tokens = model.to_tokens(prompt)\n",
        "layer = 10\n",
        "\n",
        "logits = model.run_with_hooks(\n",
        "    tokens,\n",
        "    fwd_hooks=[(get_act_name(\"resid_post\", layer), hook_fn)], # Get the name of the residual stream, and apply the hook on layer 10\n",
        ")\n",
        "next_token = logits[0, -1].argmax(dim=-1, keepdim=True) # Greedy decoding\n",
        "tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=-1)\n",
        "\n",
        "# Decode generated part\n",
        "#answer_tokens = tokens[0, initial_len:]\n",
        "answer = model.to_string(tokens)\n",
        "print(\"Steered output:\", answer) # Printing the output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkK1tMbxjiCE"
      },
      "source": [
        "💡 **Tips** 💡\n",
        "\n",
        "\n",
        "\n",
        "*   As mentioned previously, the hooks that are allowing us to access the **internal representations** of the LM are already pre-attached. To access them, we need to get their corresponding name.\n",
        "\n",
        "* Here, we need to get the residual stream of the layer (layer 10 for example) that will be passed in the hook function (`steer_residual_hook()`).\n",
        "\n",
        "* Same as before, you can get the activation name by looking at the layer names (ex : `blocks.12.hook_resid_post` would represent the 13th transformer layer's final output).\n",
        "\n",
        "As you can see, the output is not what we expected (we got \"the\" instead of \"Madrid\") ! That is because the strength (**coeff**) value we set is too small.\n",
        "\n",
        "So let's gradually increase it until we find the sweet spot !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dfb3L0C4nJMx"
      },
      "outputs": [],
      "source": [
        "for coeff in range(0,10) : # Make the strength vary between 0 and 10\n",
        "    tokens = model.to_tokens(prompt)\n",
        "    initial_len = tokens.shape[-1]\n",
        "    # Hook setup\n",
        "    def hook_fn(resid, hook):\n",
        "        return steer_residual_hook(resid, hook, direction,coeff)\n",
        "\n",
        "    logits = model.run_with_hooks(\n",
        "        tokens,\n",
        "        fwd_hooks=[(get_act_name(\"resid_post\", 10), hook_fn)], # Get the name of the residual stream, and apply the hook on layer 10\n",
        "    )\n",
        "    next_token = logits[0, -1].argmax(dim=-1, keepdim=True) # Greedy decoding\n",
        "    tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=-1)\n",
        "\n",
        "    answer = model.to_string(tokens)\n",
        "    print(\"Direction strength : \",coeff,\", Steered output:\", answer) # Printing the output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlNJJFZQpAhP"
      },
      "source": [
        "As you can see, the direction is manifesting once we reach a value of 7.\n",
        "\n",
        "Note : This little manipulation does not necessarily mean that we identified any sort of direction w.r.t the the capital (or the country). We could very well replace \"Madrid\" by \"cat\" and it would also work (I dare you to try it 🙃 )."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGrHxFR9t-6l"
      },
      "source": [
        "# 👐 Hands-on 👐 : Refusal in Language Models Is Mediated by a Single Direction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUTr01CXvD7D"
      },
      "source": [
        "In this section, we are going to put in practice what we just learned to reproduce the results of a cool mechanistic interpretability paper called :\n",
        "\n",
        "Refusal in Language Models Is Mediated by a Single Direction, from Arditi, Andy, et al\n",
        "(https://doi.org/10.48550/arXiv.2406.11717)\n",
        "\n",
        "\n",
        "This paper looks at how language models decide to refuse answering certain requests—like ones that are unsafe or inappropriate. It explores whether that refusal behavior comes from one main direction inside the model. The idea is that if this single direction can be found, it might be possible to adjust it to control how often the model refuses.\n",
        "\n",
        "And this is what we will attempt to do in this section (😈)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PlGOMoxjlba"
      },
      "source": [
        "### Preparing the data and warming-up (nothing to do here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLeei4-T6Wef"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformer_lens einops jaxtyping colorama\n",
        "!pip uninstall -y numpy\n",
        "!pip install numpy --no-cache-dir --force-reinstall\n",
        "import os\n",
        "os.kill(os.getpid(), 9)  # Force restart the runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiJuWtkVjlba"
      },
      "source": [
        "Let's import all the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vhhwl-2-jPg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import functools\n",
        "import einops\n",
        "import requests\n",
        "import io\n",
        "import textwrap\n",
        "import gc\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from torch import Tensor\n",
        "from typing import List, Callable\n",
        "from transformer_lens import HookedTransformer, utils\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from jaxtyping import Float, Int\n",
        "from colorama import Fore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXswl8qt8Zyx"
      },
      "source": [
        "For this section, we will need a much larger LLM (one that is able to give us a sharper and more precise answer, for a given request). We will use Qwen 1.8B.\n",
        "\n",
        "But really, this hands-on could be applied to any given LLM from TransformerLens. We will hzlf-precision here (fp16) to reduce the VRAM usage from the gpu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vnp65Vsg5x-5"
      },
      "outputs": [],
      "source": [
        "model = HookedTransformer.from_pretrained_no_processing('Qwen/Qwen-1_8B-chat',device='cuda',fp16=True,dtype=torch.float16,)\n",
        "\n",
        "\n",
        "model.tokenizer.padding_side = 'left'\n",
        "model.tokenizer.pad_token = '<|extra_0|>'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lI6ZoKkCu8t"
      },
      "source": [
        "To try and get this specific direction from the representation space, we will need some contrasts. Meaning we need a set of harmful, and harmless prompts, that will (or will not) induce a reject from the LLM.\n",
        "\n",
        "To do so :\n",
        "\n",
        "*  We use a subset of harmful behaviors derived from AdvBench (https://huggingface.co/datasets/walledai/AdvBench)\n",
        "*  And a subset of harmless prompts derived from Alpaca https://huggingface.co/datasets/tatsu-lab/alpaca\n",
        "\n",
        "Let's load these datasets :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i1XcVIgHEE1"
      },
      "outputs": [],
      "source": [
        "def get_harmful_instructions():\n",
        "    url = 'https://raw.githubusercontent.com/SevKod/tutorials/refs/heads/main/datasets/refusal_direction/harmful_instructions.csv'\n",
        "    response = requests.get(url)\n",
        "\n",
        "    dataset = pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n",
        "    instructions = dataset['goal'].tolist()\n",
        "\n",
        "    train, test = train_test_split(instructions, test_size=0.2, random_state=42)\n",
        "    return train, test\n",
        "\n",
        "def get_harmless_instructions():\n",
        "    url = 'https://raw.githubusercontent.com/SevKod/tutorials/refs/heads/main/datasets/refusal_direction/harmless_instructions.csv'\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Read CSV content\n",
        "    dataset = pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n",
        "\n",
        "    # Extract instructions\n",
        "    instructions = dataset['instruction'].tolist()\n",
        "\n",
        "    # Split into train/test\n",
        "    train, test = train_test_split(instructions, test_size=0.2, random_state=42)\n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rth8yvLZJsXs"
      },
      "outputs": [],
      "source": [
        "# Let's split the datasets into train and test.\n",
        "# We will use the train subset to get the direction, and apply it on the test set.\n",
        "\n",
        "harmful_inst_train, harmful_inst_test = get_harmful_instructions()\n",
        "harmless_inst_train, harmless_inst_test = get_harmless_instructions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRK2E44ejlbc"
      },
      "source": [
        "Good ! Now, let's visualize how some of these prompts look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv2ALDY_J44G"
      },
      "outputs": [],
      "source": [
        "print(\"Harmful instructions:\")\n",
        "for i in range(4):\n",
        "    print(f\"\\t{repr(harmful_inst_train[i])}\") # The spooky instructions.\n",
        "print(\"Harmless instructions:\")\n",
        "for i in range(4):\n",
        "    print(f\"\\t{repr(harmless_inst_train[i])}\") # The nice ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8WfrOwIjlbd"
      },
      "source": [
        "Alright, we have our contrast datasets, we can now find the direction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bF28LR5aDqkW"
      },
      "source": [
        "### Finding the direction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQu98gAejlbd"
      },
      "source": [
        "To obtain the refusal direction, the authors use a very simple methodology :\n",
        "\n",
        "1. Run the inference using the LLM, for both the harmless and harmful prompts.\n",
        "   \n",
        "2. Get the output representations of a given Transformer Layer for the FIRST generated token (called the POST-INSTRUCTION token in the paper) on all the inferences (for the harmful and harmless prompts).\n",
        "   \n",
        "3. Average the representations, as follow :\n",
        "\n",
        " $$\n",
        "\\boldsymbol{\\mu}_i^{(l)}=\\frac{1}{\\left|\\mathcal{D}_{\\text {harmful }}^{\\text {(train) }}\\right|} \\sum_{\\mathbf{t} \\in \\mathcal{D}_{\\text {harmful }}^{\\text {(train) }}} \\mathbf{x}_i^{(l)}(\\mathbf{t}), \\quad\n",
        "\\boldsymbol{v}_i^{(l)}=\\frac{1}{\\left|\\mathcal{D}_{\\text {harmless }}^{\\text {(train) }}\\right|} \\sum_{\\mathbf{t} \\in \\mathcal{D}_{\\text {harmless }}^{\\text {(train) }}} \\mathbf{x}_i^{(l)}(\\mathbf{t})\n",
        "$$\n",
        "\n",
        "with $\\mathbf{x}_i^{(l)}$ representing the Transformer layer's output at layer $l$ for prompt $i$.\n",
        "\n",
        "*  $\\boldsymbol{\\mu}_i^{(l)}$ represents the normalized average of all the HARMFUL prompts at a given layer $l$ for prompt $i$.\n",
        "*  $\\boldsymbol{v}_i^{(l)}$ represents the normalized average of all the HARMLESS prompts at a given layer $l$ for prompt $i$.\n",
        "*  $|\\mathcal{D}_{\\text {harmful }}^{\\text {(train) }}|$ and $|\\mathcal{D}_{\\text {harmful }}^{\\text {(train) }}|$ are the number of harmful and harmless prompts respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3Z_12i7jlbd"
      },
      "source": [
        "Qwen follows a specific chat template, with flag tokens (`<|im_start|>` and `<|im_end|>` ) indicating when an instruction or answer starts and end.\n",
        "\n",
        "The input prompt should be concatenated in-between both flags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWjdYLyJjlbe"
      },
      "outputs": [],
      "source": [
        "QWEN_CHAT_TEMPLATE = \"\"\"<|im_start|>user{instruction}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGEZ2nc3jlbe"
      },
      "source": [
        "Let's tokenize all our prompts. To make things simple, we will only work on a very small portion of our input prompts (32 prompts). To accelerate things, we will use batches (much more suited when working on datasets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8UPQSfpWOSK"
      },
      "outputs": [],
      "source": [
        "# Truncate the dataset to a small subset\n",
        "N_INST_TRAIN = 32\n",
        "harmful_inst_train = harmful_inst_train[:N_INST_TRAIN]\n",
        "harmless_inst_train = harmless_inst_train[:N_INST_TRAIN]\n",
        "\n",
        "QWEN_CHAT_TEMPLATE = \"\"\"<|im_start|>user{instruction}<|im_end|><|im_start|>assistant\"\"\"\n",
        "\n",
        "# We could do the inference on each prompt through our tokenizer and then run them independently witch cache to get the residuals,\n",
        "# However, since each input prompt has different token lengths, and we want to process everything in batches, we have to pad the sequences.\n",
        "# This is done automatically by activating \"padding=True\". This will add a specific padding tokens at the begining of the input sequence.\n",
        "\n",
        "# STEP 1 : Input the prompt into the tokenizer using the appropriate template\n",
        "\n",
        "harmful_toks = model.tokenizer(\n",
        "    [f\"<|im_start|>user\\n{inst}\\n<|im_end|>\\n<|im_start|>assistant\\n\" for inst in harmful_inst_train], # The inference prompt for all the harmful instructions (this should be a list so the tokenizer can batchify)\n",
        "    padding=True,\n",
        "    truncation=False,\n",
        "    return_tensors=\"pt\"\n",
        ").input_ids\n",
        "\n",
        "harmless_toks = model.tokenizer(\n",
        "    [f\"<|im_start|>user\\n{inst}\\n<|im_end|>\\n<|im_start|>assistant\\n\" for inst in harmless_inst_train], # The inference prompt for the harmless instructions\n",
        "    padding=True,\n",
        "    truncation=False,\n",
        "    return_tensors=\"pt\"\n",
        ").input_ids\n",
        "\n",
        "# STEP 2 : Run the inference and extract the residual streams for a given layer (ex : layer 14)\n",
        "\n",
        "print(harmful_toks.size()) # [Batch, tokens_nb]\n",
        "\n",
        "# # run model on harmful and harmless instructions, caching intermediate activations\n",
        "harmful_logits, harmful_cache = model.run_with_cache(harmful_toks)\n",
        "harmless_logits, harmless_cache = model.run_with_cache(harmless_toks)\n",
        "\n",
        "harmful_cache = harmful_cache['blocks.14.hook_resid_post'][:,-1]\n",
        "harmless_cache = harmless_cache['blocks.14.hook_resid_post'][:,-1]\n",
        "\n",
        "# STEP 3 : Get the direction\n",
        "\n",
        "harmful_mean_act = harmful_cache.mean(dim=0)\n",
        "harmless_mean_act = harmless_cache.mean(dim=0)\n",
        "refusal_dir = harmful_mean_act - harmless_mean_act\n",
        "refusal_dir = refusal_dir / refusal_dir.norm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EoxY5i1CWe3"
      },
      "source": [
        "## Ablate \"refusal direction\" via inference-time intervention\n",
        "\n",
        "Given a \"refusal direction\" $\\widehat{r} \\in \\mathbb{R}^{d_{\\text{model}}}$ with unit norm, we can ablate this direction from the model's activations $a_{l}$:\n",
        "$${a}_{l}' \\leftarrow a_l - (a_l \\cdot \\widehat{r}) \\widehat{r}$$\n",
        "\n",
        "By performing this ablation on all intermediate activations, we enforce that the model can never express this direction (or \"feature\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26rf-yncB2PT"
      },
      "outputs": [],
      "source": [
        "def get_generations(\n",
        "    model: HookedTransformer,\n",
        "    prompts: List[str],\n",
        "    max_new_tokens: int = 64,\n",
        "    batch_size: int = 4,\n",
        "    apply_direction: bool = False,\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate completions for a list of prompts using batches.\n",
        "    \"\"\"\n",
        "    generations = []\n",
        "\n",
        "    for i in tqdm(range(0, len(prompts), batch_size)):\n",
        "        batch_prompts = prompts[i:i + batch_size]\n",
        "        toks = model.to_tokens(batch_prompts)  # Direct use of model.to_tokens\n",
        "        gen_texts = generate_with_hooks( # Will do the inference (Greedy decoding with added hooks)\n",
        "            model,\n",
        "            toks,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            apply_direction = apply_direction\n",
        "        )\n",
        "        generations.extend(gen_texts)\n",
        "\n",
        "    return generations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCjd9ZWWjlbf"
      },
      "outputs": [],
      "source": [
        "def direction_ablation_hook(\n",
        "    activation: Float[Tensor, \"... d_act\"],\n",
        "    hook: HookPoint,\n",
        "    direction: Float[Tensor, \"d_act\"]\n",
        "):\n",
        "    proj = einops.einsum(activation, direction.view(-1, 1), '... d_act, d_act single -> ... single') * direction\n",
        "    return activation - proj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUgeqUltjlbf"
      },
      "outputs": [],
      "source": [
        "def generate_with_hooks(\n",
        "    model: HookedTransformer,\n",
        "    input_toks: torch.Tensor,\n",
        "    max_new_tokens: int = 64,\n",
        "    apply_direction=True\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Greedily generates tokens using model.run_with_hooks and returns decoded strings.\n",
        "    \"\"\"\n",
        "    B, L = input_toks.shape\n",
        "    all_toks = input_toks.clone()\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if apply_direction :\n",
        "            new_logits = model.run_with_hooks(all_toks, fwd_hooks=fwd_hooks)\n",
        "        else :\n",
        "            new_logits = model(all_toks)\n",
        "        next_token = new_logits[:, -1, :].argmax(dim=-1, keepdim=True)  # Greedy decoding\n",
        "        all_toks = torch.cat([all_toks, next_token], dim=1)\n",
        "\n",
        "    # Convert only the generated tokens (excluding prompt) to strings\n",
        "    return [model.to_string(toks[L:]) for toks in all_toks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-ZLJw27jlbf"
      },
      "outputs": [],
      "source": [
        "intervention_layers = list(range(model.cfg.n_layers)) # Apply our direction to all layers\n",
        "hook_fn = functools.partial(direction_ablation_hook,direction=refusal_dir) # We put our refusal direction variable in here.\n",
        "fwd_hooks = [(utils.get_act_name(act_name, l), hook_fn) for l in intervention_layers for act_name in ['resid_pre', 'resid_mid', 'resid_post']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR1G5bXoEDty"
      },
      "outputs": [],
      "source": [
        "N_INST_TEST = 32\n",
        "harmful_inst_test = harmful_inst_test[:N_INST_TEST]\n",
        "harmful_inst_test = harmful_inst_test[:N_INST_TEST]\n",
        "intervention_dir = refusal_dir\n",
        "intervention_layers = list(range(model.cfg.n_layers)) # all layers\n",
        "\n",
        "harmful_inst_test[:N_INST_TEST]\n",
        "harmful_inst_test[:N_INST_TEST]\n",
        "\n",
        "intervention_generations = get_generations(\n",
        "    model,\n",
        "    harmful_inst_test[:N_INST_TEST],\n",
        "    apply_direction=True,  # Apply the refusal direction\n",
        ")\n",
        "\n",
        "baseline_generations = get_generations(\n",
        "    model,\n",
        "    harmful_inst_test[:N_INST_TEST],\n",
        "    apply_direction=False,  # No direction ablation\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxbJr4vCFCOL"
      },
      "outputs": [],
      "source": [
        "for i in range(N_INST_TEST):\n",
        "    print(f\"INSTRUCTION {i}: {repr(harmful_inst_test[i])}\")\n",
        "    print(Fore.GREEN + f\"BASELINE COMPLETION:\")\n",
        "    print(textwrap.fill(repr(baseline_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
        "    print(Fore.RED + f\"INTERVENTION COMPLETION:\")\n",
        "    print(textwrap.fill(repr(intervention_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
        "    print(Fore.RESET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9KooaWaCDc_"
      },
      "source": [
        "## Orthogonalize weights w.r.t. \"refusal direction\"\n",
        "\n",
        "We can implement the intervention equivalently by directly orthogonalizing the weight matrices that write to the residual stream with respect to the refusal direction $\\widehat{r}$:\n",
        "$$W_{\\text{out}}' \\leftarrow W_{\\text{out}} - \\widehat{r}\\widehat{r}^{\\mathsf{T}} W_{\\text{out}}$$\n",
        "\n",
        "By orthogonalizing these weight matrices, we enforce that the model is unable to write direction $r$ to the residual stream at all!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fhx0i9vCEou"
      },
      "outputs": [],
      "source": [
        "def get_orthogonalized_matrix(W_out, direction):\n",
        "    proj = einops.einsum(W_out, direction.view(-1, 1), '... d_model, d_model single -> ... single') * direction\n",
        "    return W_out - proj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GC7cpMXZCG64"
      },
      "outputs": [],
      "source": [
        "model.W_E.data = get_orthogonalized_matrix(model.W_E, refusal_dir)\n",
        "\n",
        "for block in model.blocks:\n",
        "    block.attn.W_O.data = get_orthogonalized_matrix(block.attn.W_O, refusal_dir)\n",
        "    block.mlp.W_out.data = get_orthogonalized_matrix(block.mlp.W_out, refusal_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Y-qtouNGf3t"
      },
      "outputs": [],
      "source": [
        "orthogonalized_generations = get_generations(model, harmful_inst_test[:N_INST_TEST], apply_direction=False)  # No direction applied, but the weights are orthogonalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r68O4_4DG3P7"
      },
      "outputs": [],
      "source": [
        "for i in range(N_INST_TEST):\n",
        "    print(f\"INSTRUCTION {i}: {repr(harmful_inst_test[i])}\")\n",
        "    print(Fore.GREEN + f\"BASELINE COMPLETION:\")\n",
        "    print(textwrap.fill(repr(baseline_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
        "    print(Fore.RED + f\"INTERVENTION COMPLETION:\")\n",
        "    print(textwrap.fill(repr(intervention_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
        "    print(Fore.MAGENTA + f\"ORTHOGONALIZED COMPLETION:\")\n",
        "    print(textwrap.fill(repr(orthogonalized_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
        "    print(Fore.RESET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgj_7G1X55tm"
      },
      "source": [
        "# BONUS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2_SxOBr596W"
      },
      "source": [
        "### Activation Patching on the residual stream\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc-qKmEf6FUl"
      },
      "source": [
        "For a somewhat more involved example, let's use hooks to apply **[activation patching](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx)** on the **[Indirect Object Identification](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=iWsV3s5Kdd2ca3zNgXr5UPHa)** (IOI) task.\n",
        "\n",
        "The IOI task is the task of identifying that a sentence like \"After John and Mary went to the store, Mary gave a bottle of milk to\" continues with \" John\" rather than \" Mary\" (ie, finding the indirect object).\n",
        "\n",
        "**[Activation patching](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx)** is a technique from [Kevin Meng and David Bau's excellent ROME paper](https://rome.baulab.info/). The goal is to identify which model activations are important for completing a task. We do this by setting up a **clean prompt** and a **corrupted prompt** and a **metric** for performance on the task. We then pick a specific model activation, run the model on the corrupted prompt, but then *intervene* on that activation and patch in its value when run on the clean prompt. We then apply the metric, and see how much this patch has recovered the clean performance.\n",
        "(See [a more detailed demonstration of activation patching here](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JlOofWn6JyB"
      },
      "outputs": [],
      "source": [
        "clean_prompt = \"After John and Mary went to the store, Mary gave a bottle of milk to\"\n",
        "corrupted_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
        "\n",
        "clean_tokens = model.to_tokens(clean_prompt)\n",
        "corrupted_tokens = model.to_tokens(corrupted_prompt)\n",
        "\n",
        "def logits_to_logit_diff(logits, correct_answer=\" John\", incorrect_answer=\" Mary\"):\n",
        "    # model.to_single_token maps a string value of a single token to the token index for that token\n",
        "    # If the string is not a single token, it raises an error.\n",
        "    correct_index = model.to_single_token(correct_answer)\n",
        "    incorrect_index = model.to_single_token(incorrect_answer)\n",
        "    return logits[0, -1, correct_index] - logits[0, -1, incorrect_index]\n",
        "\n",
        "# We run on the clean prompt with the cache so we store activations to patch in later.\n",
        "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
        "clean_logit_diff = logits_to_logit_diff(clean_logits)\n",
        "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
        "\n",
        "# We don't need to cache on the corrupted prompt.\n",
        "corrupted_logits = model(corrupted_tokens)\n",
        "corrupted_logit_diff = logits_to_logit_diff(corrupted_logits)\n",
        "print(f\"Corrupted logit difference: {corrupted_logit_diff.item():.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "lgj_7G1X55tm"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}